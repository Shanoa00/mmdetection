{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66abadf4-a649-4090-8b81-4e22326ba14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mmcv.cnn import ConvModule\n",
    "from mmcv.ops import DeformConv2dPack \n",
    "from mmcv.runner import BaseModule, auto_fp16\n",
    "from timm.models.layers import PatchEmbed\n",
    "\n",
    "# from ..builder import NECKS\n",
    "\n",
    "\n",
    "# @NECKS.register_module()\n",
    "class MyNeck(BaseModule):\n",
    "    r\"\"\"Feature Pyramid Network.\n",
    "\n",
    "    This is an implementation of paper `Feature Pyramid Networks for Object\n",
    "    Detection <https://arxiv.org/abs/1612.03144>`_.\n",
    "\n",
    "    Args:\n",
    "        in_channels (list[int]): Number of input channels per scale.\n",
    "        out_channels (int): Number of output channels (used at each scale).\n",
    "        num_outs (int): Number of output scales.\n",
    "        start_level (int): Index of the start input backbone level used to\n",
    "            build the feature pyramid. Default: 0.\n",
    "        end_level (int): Index of the end input backbone level (exclusive) to\n",
    "            build the feature pyramid. Default: -1, which means the last level.\n",
    "        add_extra_convs (bool | str): If bool, it decides whether to add conv\n",
    "            layers on top of the original feature maps. Default to False.\n",
    "            If True, it is equivalent to `add_extra_convs='on_input'`.\n",
    "            If str, it specifies the source feature map of the extra convs.\n",
    "            Only the following options are allowed\n",
    "\n",
    "            - 'on_input': Last feat map of neck inputs (i.e. backbone feature).\n",
    "            - 'on_lateral': Last feature map after lateral convs.\n",
    "            - 'on_output': The last output feature map after fpn convs.\n",
    "        relu_before_extra_convs (bool): Whether to apply relu before the extra\n",
    "            conv. Default: False.\n",
    "        no_norm_on_lateral (bool): Whether to apply norm on lateral.\n",
    "            Default: False.\n",
    "        conv_cfg (dict): Config dict for convolution layer. Default: None.\n",
    "        norm_cfg (dict): Config dict for normalization layer. Default: None.\n",
    "        act_cfg (dict): Config dict for activation layer in ConvModule.\n",
    "            Default: None.\n",
    "        upsample_cfg (dict): Config dict for interpolate layer.\n",
    "            Default: dict(mode='nearest').\n",
    "        init_cfg (dict or list[dict], optional): Initialization config dict.\n",
    "\n",
    "    Example:\n",
    "        >>> import torch\n",
    "        >>> in_channels = [2, 3, 5, 7]\n",
    "        >>> scales = [340, 170, 84, 43]\n",
    "        >>> inputs = [torch.rand(1, c, s, s)\n",
    "        ...           for c, s in zip(in_channels, scales)]\n",
    "        >>> self = MyNeck(in_channels, 11, len(in_channels)).eval()\n",
    "        >>> outputs = self.forward(inputs)\n",
    "        >>> for i in range(len(outputs)):\n",
    "        ...     print(f'outputs[{i}].shape = {outputs[i].shape}')\n",
    "        outputs[0].shape = torch.Size([1, 11, 340, 340])\n",
    "        outputs[1].shape = torch.Size([1, 11, 170, 170])\n",
    "        outputs[2].shape = torch.Size([1, 11, 84, 84])\n",
    "        outputs[3].shape = torch.Size([1, 11, 43, 43])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_outs,\n",
    "                 start_level=0,\n",
    "                 end_level=-1,\n",
    "                 add_extra_convs=False,\n",
    "                 relu_before_extra_convs=False,\n",
    "                 no_norm_on_lateral=False,\n",
    "                 conv_cfg=None,\n",
    "                 norm_cfg=None,\n",
    "                 act_cfg=None,\n",
    "                 upsample_cfg=dict(mode='nearest'),\n",
    "                 init_cfg=dict(\n",
    "                     type='Xavier', layer='Conv2d', distribution='uniform')):\n",
    "        super(MyNeck, self).__init__(init_cfg)\n",
    "        assert isinstance(in_channels, list)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_ins = len(in_channels)\n",
    "        self.num_outs = num_outs\n",
    "        self.relu_before_extra_convs = relu_before_extra_convs\n",
    "        self.no_norm_on_lateral = no_norm_on_lateral\n",
    "        self.fp16_enabled = False\n",
    "        self.upsample_cfg = upsample_cfg.copy()\n",
    "\n",
    "        if end_level == -1 or end_level == self.num_ins - 1:\n",
    "            self.backbone_end_level = self.num_ins\n",
    "            assert num_outs >= self.num_ins - start_level\n",
    "        else:\n",
    "            # if end_level is not the last level, no extra level is allowed\n",
    "            self.backbone_end_level = end_level + 1\n",
    "            assert end_level < self.num_ins\n",
    "            assert num_outs == end_level - start_level + 1\n",
    "        self.start_level = start_level\n",
    "        self.end_level = end_level\n",
    "        self.add_extra_convs = add_extra_convs\n",
    "        assert isinstance(add_extra_convs, (str, bool))\n",
    "        if isinstance(add_extra_convs, str):\n",
    "            # Extra_convs_source choices: 'on_input', 'on_lateral', 'on_output'\n",
    "            assert add_extra_convs in ('on_input', 'on_lateral', 'on_output')\n",
    "        elif add_extra_convs:  # True\n",
    "            self.add_extra_convs = 'on_input'\n",
    "\n",
    "        self.lateral_convs = nn.ModuleList()\n",
    "        self.fpn_convs = nn.ModuleList()\n",
    "\n",
    "        for i in range(self.start_level, self.backbone_end_level):\n",
    "            # l_conv = ConvModule(\n",
    "            #     in_channels[i],\n",
    "            #     out_channels,\n",
    "            #     1,\n",
    "            #     conv_cfg=conv_cfg,\n",
    "            #     norm_cfg=norm_cfg if not self.no_norm_on_lateral else None,\n",
    "            #     act_cfg=act_cfg,\n",
    "            #     inplace=False)\n",
    "            l_conv = DeformConv2dPack(in_channels[i], out_channels, 1)\n",
    "            # fpn_conv = ConvModule(\n",
    "            #     out_channels,\n",
    "            #     out_channels,\n",
    "            #     3,\n",
    "            #     padding=1,\n",
    "            #     conv_cfg=conv_cfg,\n",
    "            #     norm_cfg=norm_cfg,\n",
    "            #     act_cfg=act_cfg,\n",
    "            #     inplace=False)\n",
    "            # fpn_conv= PatchEmbed(img_size=340, patch_size=2, embed_dim=in_channels[i], in_chans=in_channels[i], flatten=False)\n",
    "            fpn_conv = DeformConv2dPack(out_channels,out_channels,3,padding=1)\n",
    "\n",
    "            self.lateral_convs.append(l_conv)\n",
    "            self.fpn_convs.append(fpn_conv)\n",
    "\n",
    "        # add extra conv layers (e.g., RetinaNet)\n",
    "        extra_levels = num_outs - self.backbone_end_level + self.start_level\n",
    "        if self.add_extra_convs and extra_levels >= 1:\n",
    "            for i in range(extra_levels):\n",
    "                if i == 0 and self.add_extra_convs == 'on_input':\n",
    "                    in_channels = self.in_channels[self.backbone_end_level - 1]\n",
    "                else:\n",
    "                    in_channels = out_channels\n",
    "                extra_fpn_conv = ConvModule(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    act_cfg=act_cfg,\n",
    "                    inplace=False)\n",
    "                self.fpn_convs.append(extra_fpn_conv)\n",
    "        print(\"Fpc \", self.fpn_convs)\n",
    "\n",
    "    @auto_fp16()\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        assert len(inputs) == len(self.in_channels)\n",
    "\n",
    "        # build laterals\n",
    "        laterals = [\n",
    "            lateral_conv(inputs[i + self.start_level])\n",
    "            for i, lateral_conv in enumerate(self.lateral_convs)\n",
    "        ]\n",
    "\n",
    "        # build top-down path\n",
    "        used_backbone_levels = len(laterals)\n",
    "        for i in range(used_backbone_levels - 1, 0, -1):\n",
    "            # In some cases, fixing `scale factor` (e.g. 2) is preferred, but\n",
    "            #  it cannot co-exist with `size` in `F.interpolate`.\n",
    "            if 'scale_factor' in self.upsample_cfg:\n",
    "                # fix runtime error of \"+=\" inplace operation in PyTorch 1.10\n",
    "                laterals[i - 1] = laterals[i - 1] + F.interpolate(\n",
    "                    laterals[i], **self.upsample_cfg)\n",
    "            else:\n",
    "                prev_shape = laterals[i - 1].shape[2:]\n",
    "                laterals[i - 1] = laterals[i - 1] + F.interpolate(\n",
    "                    laterals[i], size=prev_shape, **self.upsample_cfg)\n",
    "\n",
    "        # build outputs\n",
    "        # part 1: from original levels\n",
    "        outs = [\n",
    "            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)\n",
    "        ]\n",
    "        # print(\"Fpc \", outs[0].shape)\n",
    "        # part 2: add extra levels\n",
    "        # print(\"aquiii\")\n",
    "        if self.num_outs > len(outs):\n",
    "            # use max pool to get more levels on top of outputs\n",
    "            # (e.g., Faster R-CNN, Mask R-CNN)\n",
    "            if not self.add_extra_convs:\n",
    "                for i in range(self.num_outs - used_backbone_levels):\n",
    "                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n",
    "            # add conv layers on top of original feature maps (RetinaNet)\n",
    "            else:\n",
    "                if self.add_extra_convs == 'on_input':\n",
    "                    extra_source = inputs[self.backbone_end_level - 1]\n",
    "                elif self.add_extra_convs == 'on_lateral':\n",
    "                    extra_source = laterals[-1]\n",
    "                elif self.add_extra_convs == 'on_output':\n",
    "                    extra_source = outs[-1]\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                outs.append(self.fpn_convs[used_backbone_levels](extra_source))\n",
    "                for i in range(used_backbone_levels + 1, self.num_outs):\n",
    "                    if self.relu_before_extra_convs:\n",
    "                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n",
    "                    else:\n",
    "                        outs.append(self.fpn_convs[i](outs[-1]))\n",
    "        return tuple(outs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1b707be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) OpenMMLab. All rights reserved.\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from mmcv.cnn import ConvModule\n",
    "from mmcv.runner import BaseModule, ModuleList, auto_fp16\n",
    "\n",
    "# from mmocr.models.builder import NECKS\n",
    "\n",
    "\n",
    "# @NECKS.register_module()\n",
    "class FPNF(BaseModule):\n",
    "    \"\"\"FPN-like fusion module in Shape Robust Text Detection with Progressive\n",
    "    Scale Expansion Network.\n",
    "    Args:\n",
    "        in_channels (list[int]): A list of number of input channels.\n",
    "        out_channels (int): The number of output channels.\n",
    "        fusion_type (str): Type of the final feature fusion layer. Available\n",
    "            options are \"concat\" and \"add\".\n",
    "        init_cfg (dict or list[dict], optional): Initialization configs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels=[256, 512, 44, 2048],\n",
    "                 out_channels=256,\n",
    "                 fusion_type='concat',\n",
    "                 init_cfg=dict(\n",
    "                     type='Xavier', layer='Conv2d', distribution='uniform')):\n",
    "        super().__init__(init_cfg=init_cfg)\n",
    "        conv_cfg = None\n",
    "        norm_cfg = dict(type='BN')\n",
    "        act_cfg = dict(type='ReLU')\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.lateral_convs = ModuleList()\n",
    "        self.fpn_convs = ModuleList()\n",
    "        self.backbone_end_level = len(in_channels)\n",
    "        for i in range(self.backbone_end_level):\n",
    "            l_conv = ConvModule(\n",
    "                in_channels[i],\n",
    "                out_channels,\n",
    "                1,\n",
    "                conv_cfg=conv_cfg,\n",
    "                norm_cfg=norm_cfg,\n",
    "                act_cfg=act_cfg,\n",
    "                inplace=False)\n",
    "            self.lateral_convs.append(l_conv)\n",
    "\n",
    "            if i < self.backbone_end_level - 1:\n",
    "                fpn_conv = ConvModule(\n",
    "                    out_channels,\n",
    "                    out_channels,\n",
    "                    3,\n",
    "                    padding=1,\n",
    "                    conv_cfg=conv_cfg,\n",
    "                    norm_cfg=norm_cfg,\n",
    "                    act_cfg=act_cfg,\n",
    "                    inplace=False)\n",
    "                self.fpn_convs.append(fpn_conv)\n",
    "\n",
    "        self.fusion_type = fusion_type\n",
    "\n",
    "        if self.fusion_type == 'concat':\n",
    "            feature_channels = 44\n",
    "        elif self.fusion_type == 'add':\n",
    "            feature_channels = 256\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.output_convs = ConvModule(\n",
    "            feature_channels,\n",
    "            out_channels,\n",
    "            3,\n",
    "            padding=1,\n",
    "            conv_cfg=None,\n",
    "            norm_cfg=norm_cfg,\n",
    "            act_cfg=act_cfg,\n",
    "            inplace=False)\n",
    "\n",
    "    @auto_fp16()\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (list[Tensor]): Each tensor has the shape of\n",
    "                :math:`(N, C_i, H_i, W_i)`. It usually expects 4 tensors\n",
    "                (C2-C5 features) from ResNet.\n",
    "        Returns:\n",
    "            Tensor: A tensor of shape :math:`(N, C_{out}, H_0, W_0)` where\n",
    "            :math:`C_{out}` is ``out_channels``.\n",
    "        \"\"\"\n",
    "        assert len(inputs) == len(self.in_channels)\n",
    "\n",
    "        # build laterals\n",
    "        laterals = [\n",
    "            lateral_conv(inputs[i])\n",
    "            for i, lateral_conv in enumerate(self.lateral_convs)\n",
    "        ]\n",
    "\n",
    "        # build top-down path\n",
    "        used_backbone_levels = len(laterals)\n",
    "        for i in range(used_backbone_levels - 1, 0, -1):\n",
    "            # step 1: upsample to level i-1 size and add level i-1\n",
    "            prev_shape = laterals[i - 1].shape[2:]\n",
    "            laterals[i - 1] = laterals[i - 1] + F.interpolate(\n",
    "                laterals[i], size=prev_shape, mode='nearest')\n",
    "            # step 2: smooth level i-1\n",
    "            laterals[i - 1] = self.fpn_convs[i - 1](laterals[i - 1])\n",
    "\n",
    "        # upsample and cont\n",
    "        bottom_shape = laterals[0].shape[2:]\n",
    "        for i in range(1, used_backbone_levels):\n",
    "            laterals[i] = F.interpolate(\n",
    "                laterals[i], size=bottom_shape, mode='nearest')\n",
    "\n",
    "        if self.fusion_type == 'concat':\n",
    "            out = torch.cat(laterals, 1)\n",
    "        elif self.fusion_type == 'add':\n",
    "            out = laterals[0]\n",
    "            for i in range(1, used_backbone_levels):\n",
    "                out += laterals[i]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        out = self.output_convs(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1e519dd-87a7-4032-90ff-bf3f59033cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "in_channels = [96, 192, 384, 768]\n",
    "scales = [340, 170, 84, 43]\n",
    "inputs = [torch.rand(1, c, s, s) for c, s in zip(in_channels, scales)]\n",
    "demo1= [(1,c,s,s) for c,s in zip(in_channels,scales)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e82d69-7160-42e4-bce8-0dba5679b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fpc  ModuleList(\n",
      "  (0): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (1): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (2): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (3): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "self_neck = MyNeck(in_channels, 256, num_outs=5).eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae0e451e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 340, 340])\n",
      "torch.Size([1, 192, 170, 170])\n",
      "torch.Size([1, 384, 84, 84])\n",
      "torch.Size([1, 768, 43, 43])\n"
     ]
    }
   ],
   "source": [
    "for i in inputs:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e8ff159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "# m= timm.create_model('resnet50', features_only=True, pretrained=True)\n",
    "# o= m(inputs[1])\n",
    "# for x in o:\n",
    "#     print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25b889ce-d4e1-4100-b86e-5ad5e9a7ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self2= FPNF(in_channels, 11).eval()\n",
    "# outputs2 = self2.forward(inputs)\n",
    "# for i in range(len(outputs2)):\n",
    "#     print(f'outputs[{i}].shape = {outputs2[i].shape}')\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9268b3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4100073-aa06-4021-99ea-33d15d67cd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aquiii\n",
      "outputs[0].shape = torch.Size([1, 256, 340, 340])\n",
      "outputs[1].shape = torch.Size([1, 256, 170, 170])\n",
      "outputs[2].shape = torch.Size([1, 256, 84, 84])\n",
      "outputs[3].shape = torch.Size([1, 256, 43, 43])\n",
      "outputs[4].shape = torch.Size([1, 256, 22, 22])\n"
     ]
    }
   ],
   "source": [
    "outputs = self_neck.forward(inputs)\n",
    "for i in range(len(outputs)):\n",
    "    print(f'outputs[{i}].shape = {outputs[i].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6968f1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 192, 170, 170])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[1].shape\n",
    "# inputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f64d24b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1511438/463182946.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'C' is not defined"
     ]
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09defa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11, 340, 340])\n",
      "torch.Size([1, 11, 340, 340])\n"
     ]
    }
   ],
   "source": [
    "N,C,H,W= inputs[0].shape\n",
    "l_conv = ConvModule(C, 11, 1, inplace=False)\n",
    "aaa=l_conv(inputs[0])\n",
    "print(aaa.shape)    \n",
    "# test1= torch.Conv2d(11, 11, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "fpn_conv = ConvModule(11, 11, 3, padding=1, inplace=False)\n",
    "aa1=fpn_conv(aaa)\n",
    "print(aa1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0dc5a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 96, 340, 340])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c5b96b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 96, 340, 340])\n",
      "pe torch.Size([1, 256, 170, 170])\n",
      "16.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1511438/1458687957.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                         norm_layer=nn.LayerNorm)\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#dim= num of input channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_merg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# patches = patches.contiguous().view(N, C*kernel_size*kernel_size, -1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \"\"\"\n\u001b[1;32m    335\u001b[0m         \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_resolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input feature has wrong size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"x size ({H}*{W}) are not even.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "from timm.models.layers import PatchEmbed\n",
    "from timm.models.swin_transformer import PatchMerging\n",
    "import numpy as np\n",
    "# xi = torch.randn(1, 3, 224, 224)\n",
    "xi=inputs[0]\n",
    "print(xi.shape)\n",
    "N,C,H,W= xi.shape\n",
    "\n",
    "patch_embed = PatchEmbed(img_size=H, patch_size=2, embed_dim=256, in_chans=C, flatten=False) #stage 1  (N,((h/p_s)*(h/p_s)), Emb_Dim)  pz=4, emb=96\n",
    "pe=patch_embed(xi) \n",
    "print(\"pe\", pe.shape)\n",
    "res= np.sqrt(pe.shape[1]) if np.sqrt(pe.shape[1])%2 == 0 else np.sqrt(pe.shape[1])-1\n",
    "print(res)\n",
    "patch_merg= PatchMerging(input_resolution=(int(res), int(res)), dim=11, \n",
    "                        norm_layer=nn.LayerNorm)\n",
    "#dim= num of input channels\n",
    "pm=patch_merg(pe)\n",
    "print(pm.shape)\n",
    "# patches = patches.contiguous().view(N, C*kernel_size*kernel_size, -1)\n",
    "# print(patches.shape) # [B, C*prod(kernel_size), L] as expected by Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9e5b1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3136, 3134, 94])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2= ConvModule(1,56*56,3)\n",
    "l2(x).shape\n",
    "#checar que onda con las medidas de uina imagen real porque esto esta sencillo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed8afc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 3, 3, 3, 3])\n",
      "torch.Size([3, 1, 9, 9])\n",
      "torch.Size([3, 1, 9, 9])\n",
      "torch.Size([1, 27, 9])\n",
      "torch.Size([1, 3, 170, 170])\n"
     ]
    }
   ],
   "source": [
    "# B, C, W, H = 1, 3, 170, 170\n",
    "# x = torch.randn(B, C, H, W)\n",
    "x=inputs[1]\n",
    "N,C,H,W= x.shape\n",
    "\n",
    "kernel_size = 3\n",
    "stride = 64\n",
    "patches = x.unfold(3, kernel_size, stride).unfold(2, kernel_size, stride)\n",
    "print(patches.shape) # [B, C, nb_patches_h, nb_patches_w, kernel_size, kernel_size]\n",
    "\n",
    "# perform the operations on each patch\n",
    "# ...\n",
    "\n",
    "# reshape output to match F.fold input\n",
    "patches = patches.contiguous().transpose(1,0).reshape(N, C, -1, kernel_size*kernel_size).transpose(0,1)\n",
    "print(patches.shape) # [B, C, nb_patches_all, kernel_size*kernel_size]\n",
    "patches = patches.permute(0, 1, 3, 2) \n",
    "print(patches.shape) # [B, C, kernel_size*kernel_size, nb_patches_all]\n",
    "patches = patches.contiguous().view(N, C*kernel_size*kernel_size, -1)\n",
    "print(patches.shape) # [B, C*prod(kernel_size), L] as expected by Fold\n",
    "# https://pytorch.org/docs/stable/nn.html#torch.nn.Fold\n",
    "\n",
    "output = F.fold(\n",
    "    patches, output_size=(H, W), kernel_size=kernel_size, stride=stride)\n",
    "print(output.shape) # [B, C, H, W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaabc2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1497, 0.2643, 0.4580,  ..., 0.6415, 0.8368, 0.9372],\n",
       "          [0.8278, 0.6953, 0.2105,  ..., 0.8302, 0.9618, 0.6046],\n",
       "          [0.4854, 0.7722, 0.3938,  ..., 0.9372, 0.9177, 0.4546],\n",
       "          ...,\n",
       "          [0.5123, 0.1051, 0.6526,  ..., 0.7913, 0.3375, 0.2487],\n",
       "          [0.8430, 0.3039, 0.4511,  ..., 0.2935, 0.7278, 0.1007],\n",
       "          [0.3944, 0.0967, 0.2826,  ..., 0.2515, 0.1209, 0.1671]],\n",
       "\n",
       "         [[0.9015, 0.2391, 0.8272,  ..., 0.3037, 0.4444, 0.6748],\n",
       "          [0.7975, 0.6653, 0.6949,  ..., 0.7353, 0.4852, 0.8811],\n",
       "          [0.1499, 0.4388, 0.0120,  ..., 0.9374, 0.2205, 0.5364],\n",
       "          ...,\n",
       "          [0.0409, 0.6899, 0.2713,  ..., 0.6936, 0.8746, 0.2058],\n",
       "          [0.0594, 0.1545, 0.8081,  ..., 0.7368, 0.9585, 0.7482],\n",
       "          [0.6078, 0.7251, 0.2487,  ..., 0.6656, 0.7651, 0.9039]],\n",
       "\n",
       "         [[0.1571, 0.5390, 0.6835,  ..., 0.0258, 0.9691, 0.9158],\n",
       "          [0.1764, 0.7709, 0.9666,  ..., 0.5147, 0.4872, 0.6460],\n",
       "          [0.4582, 0.4426, 0.9471,  ..., 0.6904, 0.6508, 0.0245],\n",
       "          ...,\n",
       "          [0.1728, 0.6893, 0.3693,  ..., 0.6590, 0.1809, 0.0652],\n",
       "          [0.9850, 0.2584, 0.1528,  ..., 0.8338, 0.1338, 0.9176],\n",
       "          [0.2652, 0.8642, 0.5672,  ..., 0.6102, 0.1818, 0.6435]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c725da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1497, 0.8278, 0.4854,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2643, 0.6953, 0.7722,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.4580, 0.2105, 0.3938,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.9015, 0.7975, 0.1499,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.2391, 0.6653, 0.4388,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.8272, 0.6949, 0.0120,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "         [[0.1571, 0.1764, 0.4582,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.5390, 0.7709, 0.4426,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.6835, 0.9666, 0.9471,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53f35d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 340, 340])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67be3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3136, 96])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from timm.models.layers import PatchEmbed\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "patch_embed = PatchEmbed(img_size=224, patch_size=4, embed_dim=96)\n",
    "patch_embed(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e897e5af",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [768, 3, 16, 16], expected input[1, 2, 224, 224] to have 3 channels, but got 2 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_508804/3466161560.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vit_base_patch16_224'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0moutt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dist\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# x must be a tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0mcls_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# stole cls_tokens impl from Phil Wang, thanks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/timm/models/layers/patch_embed.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Input image height ({H}) doesn't match model ({self.img_size[0]}).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Input image width ({W}) doesn't match model ({self.img_size[1]}).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# BCHW -> BNC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 443\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    444\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [768, 3, 16, 16], expected input[1, 2, 224, 224] to have 3 channels, but got 2 channels instead"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "vit=timm.create_model('vit_base_patch16_224', pretrained=False)\n",
    "vit.eval()\n",
    "outt= vit(torch.rand(1, 2, 224, 224))\n",
    "outt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee89d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
