{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb5b4ebe",
   "metadata": {},
   "source": [
    "## Just Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dfce5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/shanoa/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspace/shanoa/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n",
    "import mmcv\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8281ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Nancho_dataset/defV2/thr_range[0_2]/' #swin_nancho_deformable2\n",
    "model_name = 'swin_defor.py' #scales/swin_defor, 'top_cross_val_swin_nancho_defor.py'\n",
    "# dir= 'swin_nancho'\n",
    "# model_name= 'swin_nancho.py'\n",
    "model_load = 'mmdetection/work_dirs/'+dir+'/epoch_20.pth' #/scales, /original/best_mAP_epoch_6\n",
    "# model_load = 'mmdetection/work_dirs/'+'S03/swin_t'+'/base_best_mAP_epoch_8.pth' #usa S03 as model\n",
    "\n",
    "root_dir = '/workspace/mmdetection_mau/data/Nancho_dataset/'  #+dir+'/'  #Nancho_dataset, kuzushiji\n",
    "config_model = 'mmdetection/work_dirs/'+dir+'/'+model_name \n",
    "save_path = '/workspace/mmdetection_mau/code/mmdetection/work_dirs/'+dir+'/' #/scales/, /original/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2302a58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/matterport/Mask_RCNN/issues/2408\n",
    "# https://gist.github.com/aunsid/b28c87f98983f00163f6e588e3da1191\n",
    "## Some extra funtions\n",
    "def get_iou(a, b, epsilon=1e-5, intersection_check=False):\n",
    "    x1 = max(a[0], b[0])\n",
    "    y1 = max(a[1], b[1])\n",
    "    x2 = min(a[2], b[2])\n",
    "    y2 = min(a[3], b[3])\n",
    "    score = b[4]\n",
    "    \n",
    "    width = (x2 - x1)\n",
    "    height = (y2 - y1)\n",
    "\n",
    "    if (width < 0) or (height < 0):\n",
    "        if intersection_check:\n",
    "            return 0.0, False\n",
    "        else:\n",
    "            return 0.0\n",
    "    area_overlap = width * height\n",
    "\n",
    "    area_a = (a[2] - a[0]) * (a[3] - a[1])\n",
    "    area_b = (b[2] - b[0]) * (b[3] - b[1])\n",
    "    area_combined = area_a + area_b - area_overlap\n",
    "\n",
    "    iou = area_overlap / (area_combined + epsilon)\n",
    "    if intersection_check:\n",
    "        return iou, bool(area_overlap)\n",
    "    else:\n",
    "        return iou\n",
    "\n",
    "###todo: get mAP after plotting adding the info in the df\n",
    "def average_precision(recalls, precisions, mode='area'):\n",
    "    \"\"\"Calculate average precision (for single or multiple scales).\n",
    "\n",
    "    Args:\n",
    "        recalls (ndarray): shape (num_scales, num_dets) or (num_dets, )\n",
    "        precisions (ndarray): shape (num_scales, num_dets) or (num_dets, )\n",
    "        mode (str): 'area' or '11points', 'area' means calculating the area\n",
    "            under precision-recall curve, '11points' means calculating\n",
    "            the average precision of recalls at [0, 0.1, ..., 1]\n",
    "\n",
    "    Returns:\n",
    "        float or ndarray: calculated average precision\n",
    "    \"\"\"\n",
    "    no_scale = False\n",
    "    if recalls.ndim == 1:\n",
    "        no_scale = True\n",
    "        recalls = recalls[np.newaxis, :]\n",
    "        precisions = precisions[np.newaxis, :]\n",
    "    assert recalls.shape == precisions.shape and recalls.ndim == 2\n",
    "    num_scales = recalls.shape[0]\n",
    "    print(\"class: \",num_scales )\n",
    "    ap = np.zeros(num_scales, dtype=np.float32)\n",
    "    if mode == 'area':\n",
    "        zeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\n",
    "        ones = np.ones((num_scales, 1), dtype=recalls.dtype)\n",
    "        mrec = np.hstack((zeros, recalls, ones))\n",
    "        mpre = np.hstack((zeros, precisions, zeros))\n",
    "        for i in range(mpre.shape[1] - 1, 0, -1):\n",
    "            mpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\n",
    "        for i in range(num_scales):\n",
    "            ind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\n",
    "            ap[i] = np.sum(\n",
    "                (mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\n",
    "    elif mode == '11points':\n",
    "        for i in range(num_scales):\n",
    "            for thr in np.arange(0, 1 + 1e-3, 0.1):\n",
    "                precs = precisions[i, recalls[i, :] >= thr]\n",
    "                prec = precs.max() if precs.size > 0 else 0\n",
    "                ap[i] += prec\n",
    "        ap /= 11\n",
    "    elif mode == '101points':\n",
    "        for i in range(num_scales):\n",
    "            for thr in np.arange(0.5, .95 , 0.05):\n",
    "                precs = precisions[i, recalls[i, :] >= thr]\n",
    "                prec = precs.max() if precs.size > 0 else 0\n",
    "                ap[i] += prec\n",
    "        ap /= 101\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Unrecognized mode, only \"area\" and \"11points\" are supported')\n",
    "    if no_scale:\n",
    "        ap = ap[0]\n",
    "    return ap\n",
    "\n",
    "def calc_conditions(id, gt_boxes, pred_boxes, iou_thresh=0.5, hard_fp=True):\n",
    "    gt_class_ids_ = np.zeros(len(gt_boxes))\n",
    "    pred_class_ids_ = np.zeros(len(pred_boxes))\n",
    "\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    fppc = det_rate = 0\n",
    "    for i in range(len(gt_class_ids_)):\n",
    "        iou = []\n",
    "        for j in range(len(pred_class_ids_)):\n",
    "            now_iou, intersect = get_iou(gt_boxes[i], pred_boxes[j], intersection_check=True)\n",
    "            if now_iou >= iou_thresh and intersect:\n",
    "                iou.append(now_iou)\n",
    "                gt_class_ids_[i] = 1\n",
    "                pred_class_ids_[j] = 1\n",
    "        if len(iou) > 0: \n",
    "            tp += 1 \n",
    "            fp += len(iou) - 1\n",
    "    fn += np.count_nonzero(np.array(gt_class_ids_) == 0)\n",
    "    fp += np.count_nonzero(np.array(pred_class_ids_) == 0)\n",
    "    \n",
    "    if tp > 0:\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "        iou = tp / (tp + fp + fn)\n",
    "        fppc = fp / (tp+fn) #(tp + fp - (tp))/(tp+fn)\n",
    "        det_rate= tp / (tp + fn) #accuracy tp/(tp+fp+fn) \n",
    "        fppc= float(\"{0:.5f}\".format(fppc)) #False positives per character\n",
    "        det_rate= float(\"{0:.5f}\".format(det_rate)) #detection rate (accuracy)\n",
    "        \n",
    "    else:\n",
    "        fppc = fp / (tp+fn) #(tp + fp - (tp))/(tp+fn)\n",
    "        fppc= float(\"{0:.5f}\".format(fppc)) #False positives per character\n",
    "        precision = recall = f1_score = iou = det_rate  = 0.0\n",
    "\n",
    "    return {'image_id': id, 'fppc':fppc, 'det rate':det_rate, 'tp':tp, 'fp':fp, 'fn':fn,  \n",
    "            'precision':precision, 'recall':recall, 'f1':f1_score, 'iou':iou}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "86915715",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_file = open(root_dir+'0dtest.pkl', 'rb') #0dtest Nancho\n",
    "gt_data = pickle.load(gt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ff36b13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fpc  ModuleList(\n",
      "  (0): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (1): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (2): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (3): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      ")\n",
      "load checkpoint from local path: mmdetection/work_dirs/Nancho_dataset/defV2/thr_range[0_2]//epoch_20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 11:28:25,745 - root - INFO - DeformConv2dPack neck.fpn_convs.0 is upgraded to version 2.\n",
      "2023-03-19 11:28:25,748 - root - INFO - DeformConv2dPack neck.fpn_convs.1 is upgraded to version 2.\n",
      "2023-03-19 11:28:25,751 - root - INFO - DeformConv2dPack neck.fpn_convs.2 is upgraded to version 2.\n",
      "2023-03-19 11:28:25,753 - root - INFO - DeformConv2dPack neck.fpn_convs.3 is upgraded to version 2.\n",
      "  0%|                                                                                            | 0/22 [00:00<?, ?it/s]/workspace/mmdetection_mau/code/mmdetection/mmdet/datasets/utils.py:66: UserWarning: \"ImageToTensor\" pipeline is replaced by \"DefaultFormatBundle\" for batch inference. It is recommended to manually replace it in the test data pipeline in your config file.\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 22/22 [00:11<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donas!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "det_thres=  0.5 #0.35\n",
    "data = []\n",
    "pred_bbox =[]\n",
    "target_bbox = []\n",
    "\n",
    "try :\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "metric=MeanAveragePrecision(max_detection_thresholds=[1000])\n",
    "model = init_detector(config_model, model_load)\n",
    "for img_p in tqdm(gt_data):\n",
    "    img = mmcv.imread(root_dir + 'test_images/'+img_p['filename'])\n",
    "    pred = inference_detector(model, img)\n",
    "    data.append(calc_conditions(img_p['filename'],img_p['ann']['bboxes'],pred[1], det_thres))\n",
    "    #---get metrics\n",
    "    pred_bbox.append(pred[1]) #[:,:-1]\n",
    "    target_bbox.append(img_p['ann']['bboxes'])\n",
    "    mpreds= [dict(\n",
    "        boxes=torch.tensor(pred[1][:,:-1]),\n",
    "        scores=torch.tensor(pred[1][:,-1]),\n",
    "        labels=torch.tensor(np.ones(len(pred[1]),dtype=np.int32)),\n",
    "        )]\n",
    "    mtarget=[dict(\n",
    "        boxes=torch.tensor(img_p['ann']['bboxes']),\n",
    "        labels=torch.tensor(img_p['ann']['labels']),\n",
    "        )]\n",
    "    \n",
    "    metric.update(mpreds, mtarget)\n",
    "    # print(pred)\n",
    "    # print(\"-----\")\n",
    "    # break\n",
    "del model \n",
    "torch.cuda.empty_cache()\n",
    "print('Donas!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec1e2ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5960101598719444"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "iou= np.average(df['iou'])\n",
    "iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0255d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': tensor(-1.),\n",
      " 'map_50': tensor(0.7069),\n",
      " 'map_75': tensor(0.0556),\n",
      " 'map_large': tensor(0.2148),\n",
      " 'map_medium': tensor(0.2411),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.0274),\n",
      " 'mar_1000': tensor(0.3189),\n",
      " 'mar_1000_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.3000),\n",
      " 'mar_medium': tensor(0.3418),\n",
      " 'mar_small': tensor(0.0365)}\n"
     ]
    }
   ],
   "source": [
    "pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a478e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kuzushiji \n",
    "# FPNimage.png\n",
    "# {'map': tensor(-1.),\n",
    "#  'map_50': tensor(0.8948),\n",
    "#  'map_75': tensor(0.7596),\n",
    "#  'map_large': tensor(0.7254),\n",
    "#  'map_medium': tensor(0.5907),\n",
    "#  'map_per_class': tensor(-1.),\n",
    "#  'map_small': tensor(0.0079),\n",
    "#  'mar_1000': tensor(0image.png.6974),\n",
    "#  'mar_1000_per_class': tensor(-1.),\n",
    "#  'mar_large': tensor(0.8266),\n",
    "#  'mar_medium': tensor(0.6512),\n",
    "#  'mar_small': tensor(0.0051)}\n",
    "#def FPN\n",
    "# {'map': tensor(-1.),\n",
    "#  'map_50': tensor(0.9674),\n",
    "#  'map_75': tensor(0.8703),\n",
    "#  'map_large': tensor(0.7868),\n",
    "#  'map_medium': tensor(0.7135),\n",
    "#  'map_per_class': tensor(-1.),\n",
    "#  'map_small': tensor(0.2985),\n",
    "#  'mar_1000': tensor(0.7969),\n",
    "#  'mar_1000_per_class': tensor(-1.),\n",
    "#  'mar_large': tensor(0.8660),\n",
    "#  'mar_medium': tensor(0.7728),\n",
    "#  'mar_small': tensor(0.3729)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ca4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nancho dataset\n",
    "# def win_size=5, and scale=4\n",
    "# {'map': tensor(-1.),\n",
    "#  'map_50': tensor(0.9102),\n",
    "#  'map_75': tensor(0.2147),\n",
    "#  'map_large': tensor(0.4084),\n",
    "#  'map_medium': tensor(0.3987),\n",
    "#  'map_per_class': tensor(-1.),\n",
    "#  'map_small': tensor(0.2300),\n",
    "#  'mar_1000': tensor(0.4764),\n",
    "#  'mar_1000_per_class': tensor(-1.),\n",
    "#  'mar_large': tensor(0.4871),\n",
    "#  'mar_medium': tensor(0.4915),\n",
    "#  'mar_small': tensor(0.2863)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04c7a494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2041977/2110644519.py:48: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  recalls = recalls[np.newaxis, :]\n",
      "/tmp/ipykernel_2041977/2110644519.py:49: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  precisions = precisions[np.newaxis, :]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>fppc</th>\n",
       "      <th>det rate</th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>iou</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_1.jpg</td>\n",
       "      <td>0.078120</td>\n",
       "      <td>0.932290</td>\n",
       "      <td>179</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>0.922680</td>\n",
       "      <td>0.932292</td>\n",
       "      <td>0.927461</td>\n",
       "      <td>0.864734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_24.jpg</td>\n",
       "      <td>0.067620</td>\n",
       "      <td>0.907470</td>\n",
       "      <td>255</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>0.930657</td>\n",
       "      <td>0.907473</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_53.jpg</td>\n",
       "      <td>0.132650</td>\n",
       "      <td>0.892860</td>\n",
       "      <td>175</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>0.870647</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.881612</td>\n",
       "      <td>0.788288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_75.jpg</td>\n",
       "      <td>0.254550</td>\n",
       "      <td>0.642420</td>\n",
       "      <td>106</td>\n",
       "      <td>42</td>\n",
       "      <td>59</td>\n",
       "      <td>0.716216</td>\n",
       "      <td>0.642424</td>\n",
       "      <td>0.677316</td>\n",
       "      <td>0.512077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_48.jpg</td>\n",
       "      <td>0.098360</td>\n",
       "      <td>0.939890</td>\n",
       "      <td>172</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.939891</td>\n",
       "      <td>0.922252</td>\n",
       "      <td>0.855721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_61.jpg</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.916670</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>test_73.jpg</td>\n",
       "      <td>0.148150</td>\n",
       "      <td>0.944440</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.902655</td>\n",
       "      <td>0.822581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_8.jpg</td>\n",
       "      <td>0.093530</td>\n",
       "      <td>0.949640</td>\n",
       "      <td>132</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>0.910345</td>\n",
       "      <td>0.949640</td>\n",
       "      <td>0.929577</td>\n",
       "      <td>0.868421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_21.jpg</td>\n",
       "      <td>0.096770</td>\n",
       "      <td>0.749380</td>\n",
       "      <td>302</td>\n",
       "      <td>39</td>\n",
       "      <td>101</td>\n",
       "      <td>0.885630</td>\n",
       "      <td>0.749380</td>\n",
       "      <td>0.811828</td>\n",
       "      <td>0.683258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test_62.jpg</td>\n",
       "      <td>0.571430</td>\n",
       "      <td>0.857140</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test_15.jpg</td>\n",
       "      <td>0.066300</td>\n",
       "      <td>0.809390</td>\n",
       "      <td>293</td>\n",
       "      <td>24</td>\n",
       "      <td>69</td>\n",
       "      <td>0.924290</td>\n",
       "      <td>0.809392</td>\n",
       "      <td>0.863034</td>\n",
       "      <td>0.759067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>test_16.jpg</td>\n",
       "      <td>0.061860</td>\n",
       "      <td>0.726800</td>\n",
       "      <td>282</td>\n",
       "      <td>24</td>\n",
       "      <td>106</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.726804</td>\n",
       "      <td>0.812680</td>\n",
       "      <td>0.684466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>test_55.jpg</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>test_19.jpg</td>\n",
       "      <td>0.055870</td>\n",
       "      <td>0.849160</td>\n",
       "      <td>304</td>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>0.849162</td>\n",
       "      <td>0.891496</td>\n",
       "      <td>0.804233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>test_23.jpg</td>\n",
       "      <td>0.078430</td>\n",
       "      <td>0.960780</td>\n",
       "      <td>98</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.942308</td>\n",
       "      <td>0.890909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test_52.jpg</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.901870</td>\n",
       "      <td>193</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>0.881279</td>\n",
       "      <td>0.901869</td>\n",
       "      <td>0.891455</td>\n",
       "      <td>0.804167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>test_34.jpg</td>\n",
       "      <td>0.112030</td>\n",
       "      <td>0.821580</td>\n",
       "      <td>198</td>\n",
       "      <td>27</td>\n",
       "      <td>43</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.821577</td>\n",
       "      <td>0.849785</td>\n",
       "      <td>0.738806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>test_60.jpg</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.931030</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.805970</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>test_63.jpg</td>\n",
       "      <td>0.327870</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>58</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.834532</td>\n",
       "      <td>0.716049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>test_32.jpg</td>\n",
       "      <td>0.079440</td>\n",
       "      <td>0.915890</td>\n",
       "      <td>196</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>0.920188</td>\n",
       "      <td>0.915888</td>\n",
       "      <td>0.918033</td>\n",
       "      <td>0.848485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>test_28.jpg</td>\n",
       "      <td>0.095240</td>\n",
       "      <td>0.952380</td>\n",
       "      <td>80</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>0.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>test_46.jpg</td>\n",
       "      <td>0.049750</td>\n",
       "      <td>0.975120</td>\n",
       "      <td>196</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0.951456</td>\n",
       "      <td>0.975124</td>\n",
       "      <td>0.963145</td>\n",
       "      <td>0.928910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>promedio</td>\n",
       "      <td>0.103979</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>3321</td>\n",
       "      <td>405</td>\n",
       "      <td>574</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.852632</td>\n",
       "      <td>0.871539</td>\n",
       "      <td>0.735312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id      fppc  det rate    tp   fp   fn  precision    recall  \\\n",
       "0    test_1.jpg  0.078120  0.932290   179   15   13   0.922680  0.932292   \n",
       "1   test_24.jpg  0.067620  0.907470   255   19   26   0.930657  0.907473   \n",
       "2   test_53.jpg  0.132650  0.892860   175   26   21   0.870647  0.892857   \n",
       "3   test_75.jpg  0.254550  0.642420   106   42   59   0.716216  0.642424   \n",
       "4   test_48.jpg  0.098360  0.939890   172   18   11   0.905263  0.939891   \n",
       "5   test_61.jpg  0.500000  0.916670    11    6    1   0.647059  0.916667   \n",
       "6   test_73.jpg  0.148150  0.944440    51    8    3   0.864407  0.944444   \n",
       "7    test_8.jpg  0.093530  0.949640   132   13    7   0.910345  0.949640   \n",
       "8   test_21.jpg  0.096770  0.749380   302   39  101   0.885630  0.749380   \n",
       "9   test_62.jpg  0.571430  0.857140    12    8    2   0.600000  0.857143   \n",
       "10  test_15.jpg  0.066300  0.809390   293   24   69   0.924290  0.809392   \n",
       "11  test_16.jpg  0.061860  0.726800   282   24  106   0.921569  0.726804   \n",
       "12  test_55.jpg  8.000000  0.500000     1   16    1   0.058824  0.500000   \n",
       "13  test_19.jpg  0.055870  0.849160   304   20   54   0.938272  0.849162   \n",
       "14  test_23.jpg  0.078430  0.960780    98    8    4   0.924528  0.960784   \n",
       "15  test_52.jpg  0.121500  0.901870   193   26   21   0.881279  0.901869   \n",
       "16  test_34.jpg  0.112030  0.821580   198   27   43   0.880000  0.821577   \n",
       "17  test_60.jpg  0.379310  0.931030    27   11    2   0.710526  0.931034   \n",
       "18  test_63.jpg  0.327870  0.950820    58   20    3   0.743590  0.950820   \n",
       "19  test_32.jpg  0.079440  0.915890   196   17   18   0.920188  0.915888   \n",
       "20  test_28.jpg  0.095240  0.952380    80    8    4   0.909091  0.952381   \n",
       "21  test_46.jpg  0.049750  0.975120   196   10    5   0.951456  0.975124   \n",
       "22     promedio  0.103979  0.852632  3321  405  574   0.891304  0.852632   \n",
       "\n",
       "          f1       iou  \n",
       "0   0.927461  0.864734  \n",
       "1   0.918919  0.850000  \n",
       "2   0.881612  0.788288  \n",
       "3   0.677316  0.512077  \n",
       "4   0.922252  0.855721  \n",
       "5   0.758621  0.611111  \n",
       "6   0.902655  0.822581  \n",
       "7   0.929577  0.868421  \n",
       "8   0.811828  0.683258  \n",
       "9   0.705882  0.545455  \n",
       "10  0.863034  0.759067  \n",
       "11  0.812680  0.684466  \n",
       "12  0.105263  0.055556  \n",
       "13  0.891496  0.804233  \n",
       "14  0.942308  0.890909  \n",
       "15  0.891455  0.804167  \n",
       "16  0.849785  0.738806  \n",
       "17  0.805970  0.675000  \n",
       "18  0.834532  0.716049  \n",
       "19  0.918033  0.848485  \n",
       "20  0.930233  0.869565  \n",
       "21  0.963145  0.928910  \n",
       "22  0.871539  0.735312  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "tp=sum(df['tp'])\n",
    "fp=sum(df['fp'])\n",
    "fn=sum(df['fn'])\n",
    "fppc=fp/(tp+fn)\n",
    "dr=tp/(tp+fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = (2 * precision * recall) / (precision + recall)\n",
    "iou= np.average(df['iou'])\n",
    "ap_101= average_precision(df['recall'],df['precision'],'101points')\n",
    "\n",
    "new= {'image_id':'promedio', 'fppc':fppc, 'det rate':dr, 'tp':tp, 'fp':fp, 'fn':fn, \n",
    "        'precision':precision, 'recall':recall, 'f1':f1, 'iou':iou}  #, 'mAP101':ap_101\n",
    "df= df.append(new, ignore_index = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c67a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  1\n",
      "class:  1\n",
      "class:  1\n",
      "mAP_Area: 0.92778826\n",
      "mAP11: 0.8649603\n",
      "mAP101: 0.08478324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2041977/2110644519.py:48: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  recalls = recalls[np.newaxis, :]\n",
      "/tmp/ipykernel_2041977/2110644519.py:49: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  precisions = precisions[np.newaxis, :]\n"
     ]
    }
   ],
   "source": [
    "ap_area= average_precision(df['recall'][:-1],df['precision'][:-1],'area')\n",
    "ap_11= average_precision(df['recall'][:-1],df['precision'][:-1],'11points')\n",
    "ap_101= average_precision(df['recall'][:-1],df['precision'][:-1],'101points')\n",
    "print('mAP_Area:', ap_area)\n",
    "print('mAP11:', ap_11)\n",
    "print('mAP101:', ap_101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "715979d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987238"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_bbox[0][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b100f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['precision'][-1:]\n",
    "#def FPN\n",
    "# mAP_Area: 0.92455816\n",
    "# mAP11: 0.959152\n",
    "# mAP101: 0.08586015\n",
    "sizes_test =[[],[],[]]\n",
    "sizes_score =[[],[],[]]\n",
    "\n",
    "for ima in pred_bbox:\n",
    "    for i in ima:\n",
    "        letter_size=(i[2]-i[0])*(i[3]-i[1])\n",
    "        if letter_size < 32**2:\n",
    "            sizes_test[0].append(letter_size)\n",
    "            sizes_score[0].append(i[-1])\n",
    "        elif (letter_size>32**2) and (letter_size<96**2):\n",
    "            sizes_test[1].append(letter_size)\n",
    "            sizes_score[1].append(i[-1])\n",
    "        elif letter_size > 96**2:\n",
    "            sizes_test[2].append(letter_size)\n",
    "            sizes_score[2].append(i[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d18ff8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122.51455"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(sizes_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9b10374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70779.53"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sizes_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef52f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4064\n",
      "89023\n",
      "18696\n",
      "--Sizes--\n",
      "0.73534626\n",
      "0.9604286\n",
      "0.9733216\n"
     ]
    }
   ],
   "source": [
    "for i in sizes_test:\n",
    "    print(len(i))\n",
    "print('--Sizes--')\n",
    "for i in sizes_score:\n",
    "    print(np.average(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bd50579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4547\n",
      "87840\n",
      "18472\n"
     ]
    }
   ],
   "source": [
    "sizes_target =[[],[],[]]\n",
    "for ima in target_bbox:\n",
    "    for i in ima:\n",
    "        letter_size=(i[2]-i[0])*(i[3]-i[1])\n",
    "        if letter_size < 32**2:\n",
    "            sizes_target[0].append(letter_size)\n",
    "        elif (letter_size>32**2) and (letter_size<96**2):\n",
    "            sizes_target[1].append(letter_size)\n",
    "        elif letter_size > 96**2:\n",
    "            sizes_target[2].append(letter_size)\n",
    "\n",
    "for i in sizes_target:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dada255",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sizes_target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_116246/2397523403.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sizes_target' is not defined"
     ]
    }
   ],
   "source": [
    "min(sizes_target[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_conditions(img_p['filename'], img_p['ann']['bboxes'], pred[1], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "287ba2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_thres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da5106b",
   "metadata": {},
   "source": [
    "### Save data??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eed31a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(save_path+str(det_thres)+\"_statistics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "281e7aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fpc  ModuleList(\n",
      "  (0): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (1): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (2): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (3): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      ")\n",
      "load checkpoint from local path: mmdetection/work_dirs/HanDataset/top_best_mAP_epoch_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 19:06:07,359 - root - INFO - DeformConv2dPack neck.fpn_convs.0 is upgraded to version 2.\n",
      "2023-02-14 19:06:07,360 - root - INFO - DeformConv2dPack neck.fpn_convs.1 is upgraded to version 2.\n",
      "2023-02-14 19:06:07,360 - root - INFO - DeformConv2dPack neck.fpn_convs.2 is upgraded to version 2.\n",
      "2023-02-14 19:06:07,361 - root - INFO - DeformConv2dPack neck.fpn_convs.3 is upgraded to version 2.\n",
      "  0%|          | 0/350 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/mmdet/datasets/utils.py:66: UserWarning: \"ImageToTensor\" pipeline is replaced by \"DefaultFormatBundle\" for batch inference. It is recommended to manually replace it in the test data pipeline in your config file.\n",
      "  warnings.warn(\n",
      "100%|██████████| 350/350 [02:56<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --eval-options iou_thr=0.5\n",
    "det_thres=  0.5 #0.35, 0.5\n",
    "# data = []\n",
    "try :\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "model = init_detector(config_model, model_load)\n",
    "for img_p in tqdm(gt_data):\n",
    "    img = mmcv.imread(root_dir + 'test_images/'+img_p['filename'])\n",
    "    pred = inference_detector(model, img)\n",
    "    labs = np.ones(len(pred[1]),dtype=str)\n",
    "    # data.append(calc_conditions(img_p['filename'],img_p['ann']['bboxes'],pred[1], det_thres))\n",
    "    image = mmcv.visualization.imshow_bboxes(img,img_p['ann']['bboxes'],thickness=2, show=False)\n",
    "    mmcv.visualization.imshow_det_bboxes(image, pred[1], labels= labs, thickness=2, score_thr= det_thres,\n",
    "                                        bbox_color='red',text_color='red',font_scale=0.02, \n",
    "                                        out_file=save_path+str(det_thres)+'_shows/'+img_p['filename'],show=False)\n",
    "    # print(\"-----\")\n",
    "print(\"done!\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.core.evaluation.mean_ap import tpfp_default\n",
    "tpp, fpp = tpfp_default(pred[1],img_p['ann']['bboxes'],img_p['ann']['bboxes_ignore'], iou_thr=0.35)\n",
    "print(sum(sum(tpp)), sum(sum(fpp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions\n",
    "#pred\n",
    "#mmcv.dump(pred, save_path+'predic.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f79b0",
   "metadata": {},
   "source": [
    "Analize image cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n",
    "import mmcv\n",
    "\n",
    "import pickle\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# import required functions, classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "dir = 'S05/swin_t'\n",
    "model_name = 'top_cross_val_swin_nancho_defor.py'\n",
    "model_load = 'mmdetection/work_dirs/'+dir+'/top_best_mAP_epoch_10.pth'\n",
    "# model_load = 'mmdetection/work_dirs/'+'S03/swin_t'+'/top_best_mAP_epoch_12.pth' #usa S03 as model\n",
    "\n",
    "root_dir = '/home/mauricio/Documents/Pytorch/mmdetection/mmdetection_mau/data/S05_Detection&Recognition/'  #Nancho_dataset\n",
    "config_model = 'mmdetection/work_dirs/'+dir+'/'+model_name \n",
    "save_path = '/home/mauricio/Documents/Pytorch/mmdetection/mmdetection_mau/code/mmdetection/work_dirs/'+dir+'/'\n",
    "\n",
    "gt_file = open(root_dir+'dtest.pkl', 'rb')\n",
    "gt_data = pickle.load(gt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5270f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop2(img, w=5000, h=3000):\n",
    "    center = img.shape\n",
    "    x = center[1]/2 - w/2\n",
    "    y = center[0]/2 - h/2\n",
    "    # print(x,y)\n",
    "    img_cropped = img[int(y):int(y+h), int(x):int(x+w)]\n",
    "    return img_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd054f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99839ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    # [A.RandomResizedCrop(height=3300, width=3200, p=1)],\n",
    "    [A.Crop(1000, 200,5000,3600, p=1)],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.3, label_fields=['category_ids'])) #min_area=4500,\n",
    "\n",
    "# transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids) # category_ids=category_ids\n",
    "# visualize(\n",
    "#     transformed['image'],\n",
    "#     transformed['bboxes'],\n",
    "#     transformed['category_ids'],\n",
    "#     category_id_to_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161dedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --eval-options iou_thr=0.5\n",
    "det_thres=  0.5 #0.35, 0.5\n",
    "data = []\n",
    "try :\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "   \n",
    "model = init_detector(config_model, model_load)\n",
    "for img_p in gt_data:\n",
    "    if img_p['filename']== 'S05_027.jpg':\n",
    "        img = mmcv.imread(root_dir + 'test_images/'+img_p['filename']) #\n",
    "        bboxes = img_p['ann']['bboxes']  #albu\n",
    "        category_ids= img_p['ann']['labels'] #albu\n",
    "        # img = transform(image=img, bboxes=bboxes, category_ids=category_ids) #albu\n",
    "        # img= crop2(img, 2800, 3200)\n",
    "        pred = inference_detector(model, img) #img\n",
    "        # pred = inference_detector(model, img['image']) #albu\n",
    "        labs = np.ones(len(pred[1]),dtype=str)\n",
    "        # ###data.append(calc_conditions(img_p['filename'],img_p['ann']['bboxes'],pred[1], det_thres))\n",
    "        result= calc_conditions(img_p['filename'], img_p['ann']['bboxes'], pred[1], det_thres)  \n",
    "        image = mmcv.visualization.imshow_bboxes(img,img_p['ann']['bboxes'],thickness=2, show=False)\n",
    "        # result= calc_conditions(img_p['filename'], img['bboxes'], pred[1], det_thres)  #albu \n",
    "        # image = mmcv.visualization.imshow_bboxes(img['image'],np.array(img['bboxes']), thickness=2, show=False) #albu\n",
    "        mmcv.visualization.imshow_det_bboxes(image, pred[1], labels= labs, thickness=2, score_thr= det_thres,\n",
    "                                            bbox_color='red',text_color='red',font_scale=0.02, \n",
    "                                            out_file=save_path+'demos/'+str(det_thres)+'_'+img_p['filename'], show=False) #\n",
    "        # print(\"-----\")\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "print(result)\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299165a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sliced inference using sahi \n",
    "spec_img= 'S05_027'\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='mmdet',\n",
    "    model_path=model_load,\n",
    "    config_path=config_model,\n",
    "    confidence_threshold=det_thres,\n",
    "    image_size=1200,\n",
    "    device=\"cuda:0\", # 'cpu' or 'cuda:0'\n",
    ")\n",
    "# result = get_prediction(root_dir + 'test_images/'+spec_img+'.jpg', detection_model)\n",
    "# result.export_visuals(export_dir=save_path+'demos/sahi_'+str(det_thres)+'_'+spec_img+'.jpg')\n",
    "\n",
    "result = get_sliced_prediction(\n",
    "    root_dir + 'test_images/'+spec_img+'.jpg',\n",
    "    detection_model,\n",
    "    slice_height = 1000,\n",
    "    slice_width = 1000,\n",
    "    overlap_height_ratio = 0.2,\n",
    "    overlap_width_ratio = 0.2\n",
    ")\n",
    "# result.export_visuals(export_dir=save_path+'demos/sahi_slided_'+str(det_thres)+'_'+spec_img+'.jpg')\n",
    "print('donas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slided_pred= []\n",
    "# for i in result1.to_coco_annotations():\n",
    "#     slided_pred.append(np.array((*i['bbox'],i['score'])))\n",
    "# slided_pred=np.array((slided_pred))\n",
    "for i in result.object_prediction_list:\n",
    "    slided_pred.append(np.array((i.bbox.minx,i.bbox.miny,i.bbox.maxx, i.bbox.maxy, i.score.value)))\n",
    "slided_pred=np.array((slided_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393af92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_p in gt_data:\n",
    "    if img_p['filename']== spec_img+'.jpg':\n",
    "        img = mmcv.imread(root_dir + 'test_images/'+img_p['filename']) #\n",
    "        labs = np.ones(len(slided_pred),dtype=str)\n",
    "        image = mmcv.visualization.imshow_bboxes(img,img_p['ann']['bboxes'],thickness=2, show=False)\n",
    "        mmcv.visualization.imshow_det_bboxes(image, slided_pred, labels= labs, thickness=2, score_thr= det_thres,\n",
    "                                            bbox_color='red',text_color='red',font_scale=0.02, \n",
    "                                            out_file=save_path+'demos/sahi_'+str(det_thres)+'_'+img_p['filename'], show=False) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5144885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa= result1.object_prediction_list[0]\n",
    "# print(list((aa.bbox.minx,aa.bbox.miny,aa.bbox.maxx, aa.bbox.maxy, aa.score.value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7caf849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using S05 model:\n",
    "1280, 0.5 (00_)\n",
    "{'image_id': 'S05_027.jpg', 'fppc': 0.23767, 'det rate': 0.10762, 'tp': 24, 'fp': 53, 'fn': 199, 'precision': 0.3116883116883117, 'recall': 0.10762331838565023, 'f1': 0.16}\n",
    "1500, 0.5 (002_)\n",
    "{'image_id': 'S05_027.jpg', 'fppc': 0.20179, 'det rate': 0.4574, 'tp': 102, 'fp': 45, 'fn': 121, 'precision': 0.6938775510204082, 'recall': 0.45739910313901344, 'f1': 0.5513513513513514}\n",
    "\n",
    "\n",
    "#using S03 model:\n",
    "0.1{'image_id': 'S05_027.jpg', 'fppc': 0.05381, 'det rate': 0.83857, 'tp': 187, 'fp': 12, 'fn': 36, 'precision': 0.9396984924623115, 'recall': 0.8385650224215246, 'f1': 0.8862559241706162}\n",
    "0.5{'image_id': 'S05_027.jpg', 'fppc': 0.1704, 'det rate': 0.55605, 'tp': 124, 'fp': 38, 'fn': 99, 'precision': 0.7654320987654321, 'recall': 0.5560538116591929, 'f1': 0.6441558441558441}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0fb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_conditions(img_p['filename'], img_p['ann']['bboxes'], pred[1], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f1ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360d701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a51bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop2(img, w=5000, h=3000):\n",
    "    center = img.shape\n",
    "    x = center[1]/2 - w/2\n",
    "    y = center[0]/2 - h/2\n",
    "    # print(x,y)\n",
    "    img_cropped = img[int(y):int(y+h), int(x):int(x+w)]\n",
    "    return img_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6814be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped= crop2(image, 2800, 3200)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.hist(image.ravel(),256,[0,256]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(image, return_counts=True)\n",
    "\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ffacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = int(x_min), int(w), int(y_min), int(h)\n",
    "   \n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "    \n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c27439",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_to_name = {0: 'background', 1: 'cc'}\n",
    "for img_p in gt_data:\n",
    "    if img_p['filename']== 'S05_027.jpg':\n",
    "        # print(img_p)\n",
    "        image = cv2.imread(root_dir + 'test_images/'+img_p['filename'])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        bboxes = img_p['ann']['bboxes']\n",
    "        category_ids= img_p['ann']['labels']\n",
    "        # img = mmcv.imread(root_dir + 'test_images/'+img_p['filename']) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4b3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(category_ids)\n",
    "# len(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe8bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = A.Compose(\n",
    "#     [A.ShiftScaleRotate(p=0.5)],\n",
    "#     bbox_params=A.BboxParams(format='pascal_voc',label_fields=['category_ids']),\n",
    "# )\n",
    "transform = A.Compose(\n",
    "    # [A.RandomResizedCrop(height=3300, width=3200, p=1)],\n",
    "    [A.Crop(1000, 200,5000,3600, p=1)],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.3, label_fields=['category_ids'])) #min_area=4500,\n",
    "\n",
    "transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids) # category_ids=category_ids\n",
    "visualize(\n",
    "    transformed['image'],\n",
    "    transformed['bboxes'],\n",
    "    transformed['category_ids'],\n",
    "    category_id_to_name,\n",
    ")\n",
    "# visualize(\n",
    "#     image,\n",
    "#     bboxes,\n",
    "#     category_ids,\n",
    "#     category_id_to_name\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[1][:,:-1] #get bboxes\n",
    "(pred[1][:,-1]) #get scores\n",
    "#len(img['bboxes']) #get gt bboxes\n",
    "len(img['category_ids']) #get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62919871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_pred = [\n",
    "#     np.full(bbbox.shape[0], i, dtype=np.int32)\\\n",
    "#     for i, bbbox in enumerate(pred)\n",
    "# ]\n",
    "#pred\n",
    "tboxes= torch.tensor(pred[1][:,:-1])\n",
    "tscores= torch.tensor(pred[1][:,-1])\n",
    "tlabels= torch.tensor(np.ones(len(pred[1]),dtype=np.int32))\n",
    "#ground True\n",
    "tgt_boxes= torch.tensor(img_p['ann']['bboxes']) #img['bboxes']\n",
    "tgt_labels= torch.tensor(img_p['ann']['labels']) #img['category_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from pprint import pprint\n",
    "\n",
    "mpreds= [dict(\n",
    "    boxes=torch.tensor(pred[1][:,:-1]),\n",
    "    scores=torch.tensor(pred[1][:,-1]),\n",
    "    labels=torch.tensor(np.ones(len(pred[1]),dtype=np.int32)),\n",
    ")]\n",
    "mtarget=[dict(\n",
    "    boxes=torch.tensor(img_p['ann']['bboxes']),\n",
    "    labels=torch.tensor(img_p['ann']['labels']),\n",
    ")]\n",
    "metric=MeanAveragePrecision()\n",
    "metric.update(mpreds, mtarget)\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa246518",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MeanAveragePrecision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ebe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo 2\n",
    "preds = [\n",
    "  dict(\n",
    "    boxes=torch.tensor([[158.0, 21.0, 606.0, 285.0]]),\n",
    "    scores=torch.tensor([0.546]),\n",
    "    labels=torch.tensor([0]),\n",
    "  )\n",
    "]\n",
    "target = [\n",
    "  dict(\n",
    "    boxes=torch.tensor([[22.0, 41.0, 662.0, 345.0]]),\n",
    "    labels=torch.tensor([0]),\n",
    "  )\n",
    "]\n",
    "\n",
    "metric.update(preds, target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a7559",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.detection_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ac027",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8527b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'map': tensor(0.6000),\n",
    " 'map_50': tensor(1.),\n",
    " 'map_75': tensor(1.),\n",
    " 'map_large': tensor(0.6000),\n",
    " 'map_medium': tensor(-1.),\n",
    " 'map_per_class': tensor(-1.),\n",
    " 'map_small': tensor(-1.),\n",
    " 'mar_1': tensor(0.6000),\n",
    " 'mar_10': tensor(0.6000),\n",
    " 'mar_100': tensor(0.6000),\n",
    " 'mar_100_per_class': tensor(-1.),\n",
    " 'mar_large': tensor(0.6000),\n",
    " 'mar_medium': tensor(-1.),\n",
    " 'mar_small': tensor(-1.)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
