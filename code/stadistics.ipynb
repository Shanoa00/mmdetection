{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb5b4ebe",
   "metadata": {},
   "source": [
    "## Just Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dfce5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n",
    "import mmcv\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8281ebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'swin_kuzushiji' #swin_nancho_deformable2\n",
    "model_name = 'swin_defor.py' #scales/swin_defor, 'top_cross_val_swin_nancho_defor.py'\n",
    "# dir= 'swin_nancho'\n",
    "# model_name= 'swin_nancho.py'\n",
    "model_load = 'mmdetection/work_dirs/'+dir+'/best_mAP_epoch_31.pth' #/scales, /original/best_mAP_epoch_6\n",
    "# model_load = 'mmdetection/work_dirs/'+'S03/swin_t'+'/base_best_mAP_epoch_8.pth' #usa S03 as model\n",
    "\n",
    "root_dir = '/home/mauricio/Documents/Pytorch/mmdetection/mmdetection_mau/data/kuzushiji/'  #Nancho_dataset\n",
    "config_model = 'mmdetection/work_dirs/'+dir+'/'+model_name \n",
    "save_path = '/home/mauricio/Documents/Pytorch/mmdetection/mmdetection_mau/code/mmdetection/work_dirs/'+dir+'/' #/scales/, /original/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2302a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/matterport/Mask_RCNN/issues/2408\n",
    "# https://gist.github.com/aunsid/b28c87f98983f00163f6e588e3da1191\n",
    "## Some extra funtions\n",
    "def get_iou(a, b, epsilon=1e-5, intersection_check=False):\n",
    "    x1 = max(a[0], b[0])\n",
    "    y1 = max(a[1], b[1])\n",
    "    x2 = min(a[2], b[2])\n",
    "    y2 = min(a[3], b[3])\n",
    "    score = b[4]\n",
    "    \n",
    "    width = (x2 - x1)\n",
    "    height = (y2 - y1)\n",
    "\n",
    "    if (width < 0) or (height < 0):\n",
    "        if intersection_check:\n",
    "            return 0.0, False\n",
    "        else:\n",
    "            return 0.0\n",
    "    area_overlap = width * height\n",
    "\n",
    "    area_a = (a[2] - a[0]) * (a[3] - a[1])\n",
    "    area_b = (b[2] - b[0]) * (b[3] - b[1])\n",
    "    area_combined = area_a + area_b - area_overlap\n",
    "\n",
    "    iou = area_overlap / (area_combined + epsilon)\n",
    "    if intersection_check:\n",
    "        return iou, bool(area_overlap)\n",
    "    else:\n",
    "        return iou\n",
    "\n",
    "###todo: get mAP after plotting adding the info in the df\n",
    "def average_precision(recalls, precisions, mode='area'):\n",
    "    \"\"\"Calculate average precision (for single or multiple scales).\n",
    "\n",
    "    Args:\n",
    "        recalls (ndarray): shape (num_scales, num_dets) or (num_dets, )\n",
    "        precisions (ndarray): shape (num_scales, num_dets) or (num_dets, )\n",
    "        mode (str): 'area' or '11points', 'area' means calculating the area\n",
    "            under precision-recall curve, '11points' means calculating\n",
    "            the average precision of recalls at [0, 0.1, ..., 1]\n",
    "\n",
    "    Returns:\n",
    "        float or ndarray: calculated average precision\n",
    "    \"\"\"\n",
    "    no_scale = False\n",
    "    if recalls.ndim == 1:\n",
    "        no_scale = True\n",
    "        recalls = recalls[np.newaxis, :]\n",
    "        precisions = precisions[np.newaxis, :]\n",
    "    assert recalls.shape == precisions.shape and recalls.ndim == 2\n",
    "    num_scales = recalls.shape[0]\n",
    "    print(\"class: \",num_scales )\n",
    "    ap = np.zeros(num_scales, dtype=np.float32)\n",
    "    if mode == 'area':\n",
    "        zeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\n",
    "        ones = np.ones((num_scales, 1), dtype=recalls.dtype)\n",
    "        mrec = np.hstack((zeros, recalls, ones))\n",
    "        mpre = np.hstack((zeros, precisions, zeros))\n",
    "        for i in range(mpre.shape[1] - 1, 0, -1):\n",
    "            mpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\n",
    "        for i in range(num_scales):\n",
    "            ind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\n",
    "            ap[i] = np.sum(\n",
    "                (mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\n",
    "    elif mode == '11points':\n",
    "        for i in range(num_scales):\n",
    "            for thr in np.arange(0, 1 + 1e-3, 0.1):\n",
    "                precs = precisions[i, recalls[i, :] >= thr]\n",
    "                prec = precs.max() if precs.size > 0 else 0\n",
    "                ap[i] += prec\n",
    "        ap /= 11\n",
    "    elif mode == '101points':\n",
    "        for i in range(num_scales):\n",
    "            for thr in np.arange(0.5, .95 , 0.05):\n",
    "                precs = precisions[i, recalls[i, :] >= thr]\n",
    "                prec = precs.max() if precs.size > 0 else 0\n",
    "                ap[i] += prec\n",
    "        ap /= 101\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            'Unrecognized mode, only \"area\" and \"11points\" are supported')\n",
    "    if no_scale:\n",
    "        ap = ap[0]\n",
    "    return ap\n",
    "\n",
    "def calc_conditions(id, gt_boxes, pred_boxes, iou_thresh=0.5, hard_fp=True):\n",
    "    gt_class_ids_ = np.zeros(len(gt_boxes))\n",
    "    pred_class_ids_ = np.zeros(len(pred_boxes))\n",
    "\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    fppc = det_rate = 0\n",
    "    for i in range(len(gt_class_ids_)):\n",
    "        iou = []\n",
    "        for j in range(len(pred_class_ids_)):\n",
    "            now_iou, intersect = get_iou(gt_boxes[i], pred_boxes[j], intersection_check=True)\n",
    "            if now_iou >= iou_thresh and intersect:\n",
    "                iou.append(now_iou)\n",
    "                gt_class_ids_[i] = 1\n",
    "                pred_class_ids_[j] = 1\n",
    "        if len(iou) > 0: \n",
    "            tp += 1 \n",
    "            fp += len(iou) - 1\n",
    "    fn += np.count_nonzero(np.array(gt_class_ids_) == 0)\n",
    "    fp += np.count_nonzero(np.array(pred_class_ids_) == 0)\n",
    "    \n",
    "    if tp > 0:\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "        iou = tp / (tp + fp + fn)\n",
    "        fppc = fp / (tp+fn) #(tp + fp - (tp))/(tp+fn)\n",
    "        det_rate= tp / (tp + fn) #accuracy tp/(tp+fp+fn) \n",
    "        fppc= float(\"{0:.5f}\".format(fppc)) #False positives per character\n",
    "        det_rate= float(\"{0:.5f}\".format(det_rate)) #detection rate (accuracy)\n",
    "        \n",
    "    else:\n",
    "        fppc = fp / (tp+fn) #(tp + fp - (tp))/(tp+fn)\n",
    "        fppc= float(\"{0:.5f}\".format(fppc)) #False positives per character\n",
    "        precision = recall = f1_score = iou = det_rate  = 0.0\n",
    "\n",
    "    return {'image_id': id, 'fppc':fppc, 'det rate':det_rate, 'tp':tp, 'fp':fp, 'fn':fn,  \n",
    "            'precision':precision, 'recall':recall, 'f1':f1_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86915715",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_file = open(root_dir+'dtest.pkl', 'rb') #0dtest\n",
    "gt_data = pickle.load(gt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff36b13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fpc  ModuleList(\n",
      "  (0): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (1): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (2): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (3): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      ")\n",
      "load checkpoint from local path: mmdetection/work_dirs/swin_kuzushiji/best_mAP_epoch_31.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 17:11:54,436 - root - INFO - DeformConv2dPack neck.fpn_convs.0 is upgraded to version 2.\n",
      "2023-01-27 17:11:54,437 - root - INFO - DeformConv2dPack neck.fpn_convs.1 is upgraded to version 2.\n",
      "2023-01-27 17:11:54,438 - root - INFO - DeformConv2dPack neck.fpn_convs.2 is upgraded to version 2.\n",
      "2023-01-27 17:11:54,439 - root - INFO - DeformConv2dPack neck.fpn_convs.3 is upgraded to version 2.\n",
      "100%|██████████| 721/721 [05:16<00:00,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donas!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "det_thres=  0.5 #0.35\n",
    "data = []\n",
    "pred_bbox =[]\n",
    "target_bbox = []\n",
    "\n",
    "try :\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "metric=MeanAveragePrecision(max_detection_thresholds=[1000])\n",
    "model = init_detector(config_model, model_load)\n",
    "for img_p in tqdm(gt_data):\n",
    "    img = mmcv.imread(root_dir + 'test_images/'+img_p['filename'])\n",
    "    pred = inference_detector(model, img)\n",
    "    data.append(calc_conditions(img_p['filename'],img_p['ann']['bboxes'],pred[1], det_thres))\n",
    "    #---get metrics\n",
    "    pred_bbox.append(pred[1]) #[:,:-1]\n",
    "    target_bbox.append(img_p['ann']['bboxes'])\n",
    "    mpreds= [dict(\n",
    "        boxes=torch.tensor(pred[1][:,:-1]),\n",
    "        scores=torch.tensor(pred[1][:,-1]),\n",
    "        labels=torch.tensor(np.ones(len(pred[1]),dtype=np.int32)),\n",
    "        )]\n",
    "    mtarget=[dict(\n",
    "        boxes=torch.tensor(img_p['ann']['bboxes']),\n",
    "        labels=torch.tensor(img_p['ann']['labels']),\n",
    "        )]\n",
    "    \n",
    "    metric.update(mpreds, mtarget)\n",
    "    # print(pred)\n",
    "    # print(\"-----\")\n",
    "    # break\n",
    "del model \n",
    "torch.cuda.empty_cache()\n",
    "print('Donas!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0255d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'map': tensor(-1.),\n",
      " 'map_50': tensor(0.9674),\n",
      " 'map_75': tensor(0.8703),\n",
      " 'map_large': tensor(0.7868),\n",
      " 'map_medium': tensor(0.7135),\n",
      " 'map_per_class': tensor(-1.),\n",
      " 'map_small': tensor(0.2985),\n",
      " 'mar_1000': tensor(0.7969),\n",
      " 'mar_1000_per_class': tensor(-1.),\n",
      " 'mar_large': tensor(0.8660),\n",
      " 'mar_medium': tensor(0.7728),\n",
      " 'mar_small': tensor(0.3729)}\n"
     ]
    }
   ],
   "source": [
    "pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a478e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kuzushiji \n",
    "# FPNimage.png\n",
    "# {'map': tensor(-1.),\n",
    "#  'map_50': tensor(0.8948),\n",
    "#  'map_75': tensor(0.7596),\n",
    "#  'map_large': tensor(0.7254),\n",
    "#  'map_medium': tensor(0.5907),\n",
    "#  'map_per_class': tensor(-1.),\n",
    "#  'map_small': tensor(0.0079),\n",
    "#  'mar_1000': tensor(0image.png.6974),\n",
    "#  'mar_1000_per_class': tensor(-1.),\n",
    "#  'mar_large': tensor(0.8266),\n",
    "#  'mar_medium': tensor(0.6512),\n",
    "#  'mar_small': tensor(0.0051)}\n",
    "#def FPN\n",
    "# {'map': tensor(-1.),\n",
    "#  'map_50': tensor(0.9674),\n",
    "#  'map_75': tensor(0.8703),\n",
    "#  'map_large': tensor(0.7868),\n",
    "#  'map_medium': tensor(0.7135),\n",
    "#  'map_per_class': tensor(-1.),\n",
    "#  'map_small': tensor(0.2985),\n",
    "#  'mar_1000': tensor(0.7969),\n",
    "#  'mar_1000_per_class': tensor(-1.),\n",
    "#  'mar_large': tensor(0.8660),\n",
    "#  'mar_medium': tensor(0.7728),\n",
    "#  'mar_small': tensor(0.3729)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6ca4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nancho dataset\n",
    "# def win_size=5, and scale=4\n",
    "# {'map': tensor(-1.),\n",
    "#  'map_50': tensor(0.9102),\n",
    "#  'map_75': tensor(0.2147),\n",
    "#  'map_large': tensor(0.4084),\n",
    "#  'map_medium': tensor(0.3987),\n",
    "#  'map_per_class': tensor(-1.),\n",
    "#  'map_small': tensor(0.2300),\n",
    "#  'mar_1000': tensor(0.4764),\n",
    "#  'mar_1000_per_class': tensor(-1.),\n",
    "#  'mar_large': tensor(0.4871),\n",
    "#  'mar_medium': tensor(0.4915),\n",
    "#  'mar_small': tensor(0.2863)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04c7a494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_128153/2282637146.py:48: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  recalls = recalls[np.newaxis, :]\n",
      "/tmp/ipykernel_128153/2282637146.py:49: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  precisions = precisions[np.newaxis, :]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>fppc</th>\n",
       "      <th>det rate</th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200021869-00003_1.jpg</td>\n",
       "      <td>0.333330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>umgy011-031.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200021660-00077_1.jpg</td>\n",
       "      <td>0.020270</td>\n",
       "      <td>0.976350</td>\n",
       "      <td>289</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.979661</td>\n",
       "      <td>0.976351</td>\n",
       "      <td>0.978003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200003076_00004_2.jpg</td>\n",
       "      <td>0.019610</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>153</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.980769</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brsk002-060.jpg</td>\n",
       "      <td>0.051350</td>\n",
       "      <td>0.910810</td>\n",
       "      <td>337</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>0.946629</td>\n",
       "      <td>0.910811</td>\n",
       "      <td>0.928375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>200014740-00085_2.jpg</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>0.993130</td>\n",
       "      <td>289</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.996552</td>\n",
       "      <td>0.993127</td>\n",
       "      <td>0.994836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>200021802-00053_2.jpg</td>\n",
       "      <td>0.023260</td>\n",
       "      <td>0.967440</td>\n",
       "      <td>208</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.976526</td>\n",
       "      <td>0.967442</td>\n",
       "      <td>0.971963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>200015779_00073_1.jpg</td>\n",
       "      <td>0.020980</td>\n",
       "      <td>0.979020</td>\n",
       "      <td>280</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.979021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>200021712-00020_1.jpg</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>0.988830</td>\n",
       "      <td>177</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.988827</td>\n",
       "      <td>0.986072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>promedio</td>\n",
       "      <td>0.023681</td>\n",
       "      <td>0.976716</td>\n",
       "      <td>135408</td>\n",
       "      <td>3283</td>\n",
       "      <td>3228</td>\n",
       "      <td>0.976329</td>\n",
       "      <td>0.976716</td>\n",
       "      <td>0.976522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>722 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  image_id      fppc  det rate      tp    fp    fn  precision  \\\n",
       "0    200021869-00003_1.jpg  0.333330  1.000000       3     1     0   0.750000   \n",
       "1          umgy011-031.jpg  0.000000  1.000000     184     0     0   1.000000   \n",
       "2    200021660-00077_1.jpg  0.020270  0.976350     289     6     7   0.979661   \n",
       "3    200003076_00004_2.jpg  0.019610  1.000000     153     3     0   0.980769   \n",
       "4          brsk002-060.jpg  0.051350  0.910810     337    19    33   0.946629   \n",
       "..                     ...       ...       ...     ...   ...   ...        ...   \n",
       "717  200014740-00085_2.jpg  0.003440  0.993130     289     1     2   0.996552   \n",
       "718  200021802-00053_2.jpg  0.023260  0.967440     208     5     7   0.976526   \n",
       "719  200015779_00073_1.jpg  0.020980  0.979020     280     6     6   0.979021   \n",
       "720  200021712-00020_1.jpg  0.016760  0.988830     177     3     2   0.983333   \n",
       "721               promedio  0.023681  0.976716  135408  3283  3228   0.976329   \n",
       "\n",
       "       recall        f1  \n",
       "0    1.000000  0.857143  \n",
       "1    1.000000  1.000000  \n",
       "2    0.976351  0.978003  \n",
       "3    1.000000  0.990291  \n",
       "4    0.910811  0.928375  \n",
       "..        ...       ...  \n",
       "717  0.993127  0.994836  \n",
       "718  0.967442  0.971963  \n",
       "719  0.979021  0.979021  \n",
       "720  0.988827  0.986072  \n",
       "721  0.976716  0.976522  \n",
       "\n",
       "[722 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "tp=sum(df['tp'])\n",
    "fp=sum(df['fp'])\n",
    "fn=sum(df['fn'])\n",
    "fppc=fp/(tp+fn)\n",
    "dr=tp/(tp+fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = (2 * precision * recall) / (precision + recall)\n",
    "ap_101= average_precision(df['recall'],df['precision'],'101points')\n",
    "\n",
    "new= {'image_id':'promedio', 'fppc':fppc, 'det rate':dr, 'tp':tp, 'fp':fp, 'fn':fn, \n",
    "        'precision':precision, 'recall':recall, 'f1':f1}  #, 'mAP101':ap_101\n",
    "df= df.append(new, ignore_index = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c67a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class:  1\n",
      "class:  1\n",
      "class:  1\n",
      "mAP_Area: 0.9889222\n",
      "mAP11: 1.0\n",
      "mAP101: 0.089108914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_128153/2282637146.py:48: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  recalls = recalls[np.newaxis, :]\n",
      "/tmp/ipykernel_128153/2282637146.py:49: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  precisions = precisions[np.newaxis, :]\n"
     ]
    }
   ],
   "source": [
    "ap_area= average_precision(df['recall'][:-1],df['precision'][:-1],'area')\n",
    "ap_11= average_precision(df['recall'][:-1],df['precision'][:-1],'11points')\n",
    "ap_101= average_precision(df['recall'][:-1],df['precision'][:-1],'101points')\n",
    "print('mAP_Area:', ap_area)\n",
    "print('mAP11:', ap_11)\n",
    "print('mAP101:', ap_101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "715979d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9992442"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_bbox[0][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b100f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['precision'][-1:]\n",
    "#def FPN\n",
    "# mAP_Area: 0.92455816\n",
    "# mAP11: 0.959152\n",
    "# mAP101: 0.08586015\n",
    "sizes_test =[[],[],[]]\n",
    "sizes_score =[[],[],[]]\n",
    "\n",
    "for ima in pred_bbox:\n",
    "    for i in ima:\n",
    "        letter_size=(i[2]-i[0])*(i[3]-i[1])\n",
    "        if letter_size < 32**2:\n",
    "            sizes_test[0].append(letter_size)\n",
    "            sizes_score[0].append(i[-1])\n",
    "        elif (letter_size>32**2) and (letter_size<96**2):\n",
    "            sizes_test[1].append(letter_size)\n",
    "            sizes_score[1].append(i[-1])\n",
    "        elif letter_size > 96**2:\n",
    "            sizes_test[2].append(letter_size)\n",
    "            sizes_score[2].append(i[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d18ff8bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9216.112"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(sizes_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9b10374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023.4497"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sizes_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef52f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623\n",
      "94751\n",
      "43292\n",
      "--Sizes--\n",
      "0.47874758\n",
      "0.8650953\n",
      "0.95150065\n"
     ]
    }
   ],
   "source": [
    "for i in sizes_test:\n",
    "    print(len(i))\n",
    "print('--Sizes--')\n",
    "for i in sizes_score:\n",
    "    print(np.average(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bd50579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976\n",
      "97459\n",
      "40151\n"
     ]
    }
   ],
   "source": [
    "sizes_target =[[],[],[]]\n",
    "for ima in target_bbox:\n",
    "    for i in ima:\n",
    "        letter_size=(i[2]-i[0])*(i[3]-i[1])\n",
    "        if letter_size < 32**2:\n",
    "            sizes_target[0].append(letter_size)\n",
    "        elif (letter_size>32**2) and (letter_size<96**2):\n",
    "            sizes_target[1].append(letter_size)\n",
    "        elif letter_size > 96**2:\n",
    "            sizes_target[2].append(letter_size)\n",
    "\n",
    "for i in sizes_target:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dada255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9222.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(sizes_target[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d812b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_conditions(img_p['filename'], img_p['ann']['bboxes'], pred[1], 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da5106b",
   "metadata": {},
   "source": [
    "### Save data??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eed31a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(save_path+str(det_thres)+\"_statistics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "281e7aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fpc  ModuleList(\n",
      "  (0): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (1): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (2): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      "  (3): DeformConv2dPack(in_channels=256,\n",
      "  out_channels=256,\n",
      "  kernel_size=(3, 3),\n",
      "  stride=(1, 1),\n",
      "  padding=(1, 1),\n",
      "  dilation=(1, 1),\n",
      "  groups=1,\n",
      "  deform_groups=1,\n",
      "  bias=False)\n",
      ")\n",
      "load checkpoint from local path: mmdetection/work_dirs/swin_kuzushiji/best_mAP_epoch_31.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 17:04:23,011 - root - INFO - DeformConv2dPack neck.fpn_convs.0 is upgraded to version 2.\n",
      "2023-01-27 17:04:23,012 - root - INFO - DeformConv2dPack neck.fpn_convs.1 is upgraded to version 2.\n",
      "2023-01-27 17:04:23,013 - root - INFO - DeformConv2dPack neck.fpn_convs.2 is upgraded to version 2.\n",
      "2023-01-27 17:04:23,014 - root - INFO - DeformConv2dPack neck.fpn_convs.3 is upgraded to version 2.\n",
      "100%|██████████| 721/721 [04:52<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --eval-options iou_thr=0.5\n",
    "det_thres=  0.5 #0.35, 0.5\n",
    "# data = []\n",
    "try :\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "model = init_detector(config_model, model_load)\n",
    "for img_p in tqdm(gt_data):\n",
    "    img = mmcv.imread(root_dir + 'test_images/'+img_p['filename'])\n",
    "    pred = inference_detector(model, img)\n",
    "    labs = np.ones(len(pred[1]),dtype=str)\n",
    "    # data.append(calc_conditions(img_p['filename'],img_p['ann']['bboxes'],pred[1], det_thres))\n",
    "    image = mmcv.visualization.imshow_bboxes(img,img_p['ann']['bboxes'],thickness=2, show=False)\n",
    "    mmcv.visualization.imshow_det_bboxes(image, pred[1], labels= labs, thickness=2, score_thr= det_thres,\n",
    "                                        bbox_color='red',text_color='red',font_scale=0.02, \n",
    "                                        out_file=save_path+str(det_thres)+'_shows/'+img_p['filename'],show=False)\n",
    "    # print(\"-----\")\n",
    "print(\"done!\")\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d7e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.core.evaluation.mean_ap import tpfp_default\n",
    "tpp, fpp = tpfp_default(pred[1],img_p['ann']['bboxes'],img_p['ann']['bboxes_ignore'], iou_thr=0.35)\n",
    "print(sum(sum(tpp)), sum(sum(fpp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e8b9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save predictions\n",
    "#pred\n",
    "#mmcv.dump(pred, save_path+'predic.pkl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b98f79b0",
   "metadata": {},
   "source": [
    "Analize image cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0958a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n",
    "import mmcv\n",
    "\n",
    "import pickle\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# import required functions, classes\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "dir = 'S05/swin_t'\n",
    "model_name = 'top_cross_val_swin_nancho_defor.py'\n",
    "model_load = 'mmdetection/work_dirs/'+dir+'/top_best_mAP_epoch_10.pth'\n",
    "# model_load = 'mmdetection/work_dirs/'+'S03/swin_t'+'/top_best_mAP_epoch_12.pth' #usa S03 as model\n",
    "\n",
    "root_dir = '/home/mauricio/Documents/Pytorch/mmdetection/mmdetection_mau/data/S05_Detection&Recognition/'  #Nancho_dataset\n",
    "config_model = 'mmdetection/work_dirs/'+dir+'/'+model_name \n",
    "save_path = '/home/mauricio/Documents/Pytorch/mmdetection/mmdetection_mau/code/mmdetection/work_dirs/'+dir+'/'\n",
    "\n",
    "gt_file = open(root_dir+'dtest.pkl', 'rb')\n",
    "gt_data = pickle.load(gt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5270f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop2(img, w=5000, h=3000):\n",
    "    center = img.shape\n",
    "    x = center[1]/2 - w/2\n",
    "    y = center[0]/2 - h/2\n",
    "    # print(x,y)\n",
    "    img_cropped = img[int(y):int(y+h), int(x):int(x+w)]\n",
    "    return img_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd054f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99839ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    # [A.RandomResizedCrop(height=3300, width=3200, p=1)],\n",
    "    [A.Crop(1000, 200,5000,3600, p=1)],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.3, label_fields=['category_ids'])) #min_area=4500,\n",
    "\n",
    "# transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids) # category_ids=category_ids\n",
    "# visualize(\n",
    "#     transformed['image'],\n",
    "#     transformed['bboxes'],\n",
    "#     transformed['category_ids'],\n",
    "#     category_id_to_name,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161dedbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --eval-options iou_thr=0.5\n",
    "det_thres=  0.5 #0.35, 0.5\n",
    "data = []\n",
    "try :\n",
    "    del model\n",
    "except NameError:\n",
    "    pass\n",
    "   \n",
    "model = init_detector(config_model, model_load)\n",
    "for img_p in gt_data:\n",
    "    if img_p['filename']== 'S05_027.jpg':\n",
    "        img = mmcv.imread(root_dir + 'test_images/'+img_p['filename']) #\n",
    "        bboxes = img_p['ann']['bboxes']  #albu\n",
    "        category_ids= img_p['ann']['labels'] #albu\n",
    "        # img = transform(image=img, bboxes=bboxes, category_ids=category_ids) #albu\n",
    "        # img= crop2(img, 2800, 3200)\n",
    "        pred = inference_detector(model, img) #img\n",
    "        # pred = inference_detector(model, img['image']) #albu\n",
    "        labs = np.ones(len(pred[1]),dtype=str)\n",
    "        # ###data.append(calc_conditions(img_p['filename'],img_p['ann']['bboxes'],pred[1], det_thres))\n",
    "        result= calc_conditions(img_p['filename'], img_p['ann']['bboxes'], pred[1], det_thres)  \n",
    "        image = mmcv.visualization.imshow_bboxes(img,img_p['ann']['bboxes'],thickness=2, show=False)\n",
    "        # result= calc_conditions(img_p['filename'], img['bboxes'], pred[1], det_thres)  #albu \n",
    "        # image = mmcv.visualization.imshow_bboxes(img['image'],np.array(img['bboxes']), thickness=2, show=False) #albu\n",
    "        mmcv.visualization.imshow_det_bboxes(image, pred[1], labels= labs, thickness=2, score_thr= det_thres,\n",
    "                                            bbox_color='red',text_color='red',font_scale=0.02, \n",
    "                                            out_file=save_path+'demos/'+str(det_thres)+'_'+img_p['filename'], show=False) #\n",
    "        # print(\"-----\")\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "print(result)\n",
    "print(\"Done!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299165a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Sliced inference using sahi \n",
    "spec_img= 'S05_027'\n",
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='mmdet',\n",
    "    model_path=model_load,\n",
    "    config_path=config_model,\n",
    "    confidence_threshold=det_thres,\n",
    "    image_size=1200,\n",
    "    device=\"cuda:0\", # 'cpu' or 'cuda:0'\n",
    ")\n",
    "# result = get_prediction(root_dir + 'test_images/'+spec_img+'.jpg', detection_model)\n",
    "# result.export_visuals(export_dir=save_path+'demos/sahi_'+str(det_thres)+'_'+spec_img+'.jpg')\n",
    "\n",
    "result = get_sliced_prediction(\n",
    "    root_dir + 'test_images/'+spec_img+'.jpg',\n",
    "    detection_model,\n",
    "    slice_height = 1000,\n",
    "    slice_width = 1000,\n",
    "    overlap_height_ratio = 0.2,\n",
    "    overlap_width_ratio = 0.2\n",
    ")\n",
    "# result.export_visuals(export_dir=save_path+'demos/sahi_slided_'+str(det_thres)+'_'+spec_img+'.jpg')\n",
    "print('donas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slided_pred= []\n",
    "# for i in result1.to_coco_annotations():\n",
    "#     slided_pred.append(np.array((*i['bbox'],i['score'])))\n",
    "# slided_pred=np.array((slided_pred))\n",
    "for i in result.object_prediction_list:\n",
    "    slided_pred.append(np.array((i.bbox.minx,i.bbox.miny,i.bbox.maxx, i.bbox.maxy, i.score.value)))\n",
    "slided_pred=np.array((slided_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393af92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_p in gt_data:\n",
    "    if img_p['filename']== spec_img+'.jpg':\n",
    "        img = mmcv.imread(root_dir + 'test_images/'+img_p['filename']) #\n",
    "        labs = np.ones(len(slided_pred),dtype=str)\n",
    "        image = mmcv.visualization.imshow_bboxes(img,img_p['ann']['bboxes'],thickness=2, show=False)\n",
    "        mmcv.visualization.imshow_det_bboxes(image, slided_pred, labels= labs, thickness=2, score_thr= det_thres,\n",
    "                                            bbox_color='red',text_color='red',font_scale=0.02, \n",
    "                                            out_file=save_path+'demos/sahi_'+str(det_thres)+'_'+img_p['filename'], show=False) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5144885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa= result1.object_prediction_list[0]\n",
    "# print(list((aa.bbox.minx,aa.bbox.miny,aa.bbox.maxx, aa.bbox.maxy, aa.score.value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7caf849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using S05 model:\n",
    "1280, 0.5 (00_)\n",
    "{'image_id': 'S05_027.jpg', 'fppc': 0.23767, 'det rate': 0.10762, 'tp': 24, 'fp': 53, 'fn': 199, 'precision': 0.3116883116883117, 'recall': 0.10762331838565023, 'f1': 0.16}\n",
    "1500, 0.5 (002_)\n",
    "{'image_id': 'S05_027.jpg', 'fppc': 0.20179, 'det rate': 0.4574, 'tp': 102, 'fp': 45, 'fn': 121, 'precision': 0.6938775510204082, 'recall': 0.45739910313901344, 'f1': 0.5513513513513514}\n",
    "\n",
    "\n",
    "#using S03 model:\n",
    "0.1{'image_id': 'S05_027.jpg', 'fppc': 0.05381, 'det rate': 0.83857, 'tp': 187, 'fp': 12, 'fn': 36, 'precision': 0.9396984924623115, 'recall': 0.8385650224215246, 'f1': 0.8862559241706162}\n",
    "0.5{'image_id': 'S05_027.jpg', 'fppc': 0.1704, 'det rate': 0.55605, 'tp': 124, 'fp': 38, 'fn': 99, 'precision': 0.7654320987654321, 'recall': 0.5560538116591929, 'f1': 0.6441558441558441}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0fb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_conditions(img_p['filename'], img_p['ann']['bboxes'], pred[1], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f1ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da1e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360d701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a51bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop2(img, w=5000, h=3000):\n",
    "    center = img.shape\n",
    "    x = center[1]/2 - w/2\n",
    "    y = center[0]/2 - h/2\n",
    "    # print(x,y)\n",
    "    img_cropped = img[int(y):int(y+h), int(x):int(x+w)]\n",
    "    return img_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6814be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped= crop2(image, 2800, 3200)\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "plt.hist(image.ravel(),256,[0,256]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b09d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(image, return_counts=True)\n",
    "\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ffacb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOX_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = int(x_min), int(w), int(y_min), int(h)\n",
    "   \n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "    \n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c27439",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_id_to_name = {0: 'background', 1: 'cc'}\n",
    "for img_p in gt_data:\n",
    "    if img_p['filename']== 'S05_027.jpg':\n",
    "        # print(img_p)\n",
    "        image = cv2.imread(root_dir + 'test_images/'+img_p['filename'])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        bboxes = img_p['ann']['bboxes']\n",
    "        category_ids= img_p['ann']['labels']\n",
    "        # img = mmcv.imread(root_dir + 'test_images/'+img_p['filename']) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4b3e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(category_ids)\n",
    "# len(bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe8bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = A.Compose(\n",
    "#     [A.ShiftScaleRotate(p=0.5)],\n",
    "#     bbox_params=A.BboxParams(format='pascal_voc',label_fields=['category_ids']),\n",
    "# )\n",
    "transform = A.Compose(\n",
    "    # [A.RandomResizedCrop(height=3300, width=3200, p=1)],\n",
    "    [A.Crop(1000, 200,5000,3600, p=1)],\n",
    "    bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.3, label_fields=['category_ids'])) #min_area=4500,\n",
    "\n",
    "transformed = transform(image=image, bboxes=bboxes, category_ids=category_ids) # category_ids=category_ids\n",
    "visualize(\n",
    "    transformed['image'],\n",
    "    transformed['bboxes'],\n",
    "    transformed['category_ids'],\n",
    "    category_id_to_name,\n",
    ")\n",
    "# visualize(\n",
    "#     image,\n",
    "#     bboxes,\n",
    "#     category_ids,\n",
    "#     category_id_to_name\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[1][:,:-1] #get bboxes\n",
    "(pred[1][:,-1]) #get scores\n",
    "#len(img['bboxes']) #get gt bboxes\n",
    "len(img['category_ids']) #get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62919871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_pred = [\n",
    "#     np.full(bbbox.shape[0], i, dtype=np.int32)\\\n",
    "#     for i, bbbox in enumerate(pred)\n",
    "# ]\n",
    "#pred\n",
    "tboxes= torch.tensor(pred[1][:,:-1])\n",
    "tscores= torch.tensor(pred[1][:,-1])\n",
    "tlabels= torch.tensor(np.ones(len(pred[1]),dtype=np.int32))\n",
    "#ground True\n",
    "tgt_boxes= torch.tensor(img_p['ann']['bboxes']) #img['bboxes']\n",
    "tgt_labels= torch.tensor(img_p['ann']['labels']) #img['category_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from pprint import pprint\n",
    "\n",
    "mpreds= [dict(\n",
    "    boxes=torch.tensor(pred[1][:,:-1]),\n",
    "    scores=torch.tensor(pred[1][:,-1]),\n",
    "    labels=torch.tensor(np.ones(len(pred[1]),dtype=np.int32)),\n",
    ")]\n",
    "mtarget=[dict(\n",
    "    boxes=torch.tensor(img_p['ann']['bboxes']),\n",
    "    labels=torch.tensor(img_p['ann']['labels']),\n",
    ")]\n",
    "metric=MeanAveragePrecision()\n",
    "metric.update(mpreds, mtarget)\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa246518",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MeanAveragePrecision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6ebe1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demo 2\n",
    "preds = [\n",
    "  dict(\n",
    "    boxes=torch.tensor([[158.0, 21.0, 606.0, 285.0]]),\n",
    "    scores=torch.tensor([0.546]),\n",
    "    labels=torch.tensor([0]),\n",
    "  )\n",
    "]\n",
    "target = [\n",
    "  dict(\n",
    "    boxes=torch.tensor([[22.0, 41.0, 662.0, 345.0]]),\n",
    "    labels=torch.tensor([0]),\n",
    "  )\n",
    "]\n",
    "\n",
    "metric.update(preds, target)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516a7559",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.detection_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156ac027",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(metric.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8527b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'map': tensor(0.6000),\n",
    " 'map_50': tensor(1.),\n",
    " 'map_75': tensor(1.),\n",
    " 'map_large': tensor(0.6000),\n",
    " 'map_medium': tensor(-1.),\n",
    " 'map_per_class': tensor(-1.),\n",
    " 'map_small': tensor(-1.),\n",
    " 'mar_1': tensor(0.6000),\n",
    " 'mar_10': tensor(0.6000),\n",
    " 'mar_100': tensor(0.6000),\n",
    " 'mar_100_per_class': tensor(-1.),\n",
    " 'mar_large': tensor(0.6000),\n",
    " 'mar_medium': tensor(-1.),\n",
    " 'mar_small': tensor(-1.)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
